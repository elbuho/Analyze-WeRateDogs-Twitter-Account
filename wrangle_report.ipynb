{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>WRANGLE REPORT<center>\n",
    "<center>By: Jorge Mu√±oz Rama<center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "* The objective of this report is to describe the effort made to gather, access and store data from the Twitter account called WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collected in this project comes from three files:\n",
    "* The twitter-archive_enhance.csv and tweet-json.txt files; which have been provided by Udacity and we have opened them programmatically using pandas.read_csv method.\n",
    "* The image-predictions.tsv file, which we have downloaded programmatically from the link https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv', to do this we have used the request package and pandas.read_csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "\n",
    "We used different methods like info(); isna().sum(), Values-count(), duplicated(), groupBy, query, str.extractall, head() and tail().\n",
    "The principal issues that we found was:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality\n",
    "* timestamp column is a string, it should be datatime.\n",
    "* The source colum has 4 diferent type of source but the information is among html tags.\n",
    "* The column text are retweet and all of them star with (RT @) we are going to drop them\n",
    "* The column name didn't have NAN, We can see that there are 745 None values in this column. That amount of NAN (None) is huge and at the same times there are value like a, an, the, which are not names, all of them start with lowercase, 109 names that star with lowercase where none of them are dog names. They extracted the names of the dog from the column text after the phrases \"this is\" or \"here is\", but some names , appear right after 'named.\n",
    "* In expanded_urls, there are some repeated url in the same field or doesn't star with https://twitter.com/. We are going to clean this column extracting the correct patron like https://twitter.com/dog_rates/status/889531135344209921/photo/1 or video.\n",
    "* We find values in the rating_numerator less than 10 and some values so high like 1776, 960 or 666.\n",
    "* The rating-denominator had some values in the denominator different than 10.\n",
    "* We found that in the column text there is a patron where the rating is at the end of the tweet and also there are some values in the numerator in the text column with decimal places that is causing problems as well as we found in the same tweet more the one rating patron. It seem that they extracted the rating from the first one.\n",
    "* We saw that some of the row in the column text are retweet and all of them star with (RT @) we are going to drop them\n",
    "* image_prediction has 66 duplicated jpg_url (imges).\n",
    "* We got 324 rows where all prediction(p1_dog, p2_dog and p3_dog) are false, some prediction are not dog.\n",
    "\n",
    "#### Tidiness\n",
    "* doggo, floofer, pupper and puppo columns, we created a new column stage and traid to get more data from de text column. We saw that some tweets have more the one dog in the picture and more the one stage, so we trie to get that form the column text.\n",
    "* We  used rating_numerator and rating_denominator to create a new column called rating.\n",
    "* merging the three data frame in only one dataframe by tweet_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make a copy of the three dataframe\n",
    "* Get the information among html tags in the source column.\n",
    "* Convert timestamp column to datatime\n",
    "* We extracted from tex column the stage of the dog. In our case we got those stage: pupper,doggo, puppo, floofer,  puppers,doggos, puppos, puppo-doggo, pupper-doggo, doggo-floofer. We made that clasification because there are more than one dog in the tweet with more of one stage.\n",
    "* We created a new column called stage.\n",
    "* We drop the columns pupper,doggo, puppo,floofer.convert None value by nan in name column.\n",
    "* convert None value by nan in name column.\n",
    "* Change the format of the name column to lowrcase\n",
    "* We got more names frome the text column after the word named.\n",
    "* We extrated from expanded_urls just valid link like this format https://twitter.com/dog_rates/status/670444955656130560/photo/1.\n",
    "* We cleaned some row in the column expanded_urls where there were a repeated valid url.\n",
    "* We droped rows where expanded_urls have nan's (not images).\n",
    "* we got all the possible rating in the column text. The last at the final of the tweet is the valid rating, to do that we used str.findall().\n",
    "* We split into two new column numerator and denominator and made equal to the rating:numerator and rating_denominator and after that we drop numerator and denominator.\n",
    "* We saw that the row 516 doesn't have rating, 24/7 is a date and the are a rating_numerator value equal to 1776 which is a outlier we droped those rows.\n",
    "* We droped all the rows in the column text where RT @, because are retweets.\n",
    "* We change to lowercase in the image_clean dataframe the column p1,p2 and p3.\n",
    "* We droped 66 duplicated values rows in jpg_url column.\n",
    "* There are 318 rows where p1_dog, p2_dog and p3_dog are False, they are not dogs or he algorithm is no able to recognize as dog the image. we droped them.\n",
    "* We used wide_to_long panda function, but firstly we have changed the name of the column. The idea is to have just 3 columns (p, p_conf and p_dog).\n",
    "* We grouped by tweet_id and chose the last one of each tweet_id group, using last().\n",
    "* We changed in witter_count_clean the tweet_id column using astype(int).\n",
    "* Merge archive_clean. with twitter_count_clean,  on='tweet_id'. and created archive_master dataFrame.\n",
    "* we chcked there were not 'favorite_count == 0'.\n",
    "* We merged archive_master and image_clean.\n",
    "* we renamed the columns in archive_master before to store  the dataFrame\n",
    "* And finally we stored the archive_master as csv file named 'twitter_archive_master.csv'."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
